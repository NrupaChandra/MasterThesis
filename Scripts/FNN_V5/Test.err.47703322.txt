Lmod: loading gcc 8.5.0 
Lmod: loading cuda 11.8 
Mon Mar  3 16:50:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:C6:00.0 Off |                    0 |
| N/A   22C    P0             66W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[I 2025-03-03 16:50:49,024] A new study created in memory with name: no-name-2d9084f1-cd65-43d5-bef4-9b8210a51d92
/work/home/ng66sume/MasterThesis/Scripts/FNN_V5/train_fnn.py:246: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  dropout_rate = trial.suggest_uniform("dropout_rate", 0.0, 0.5) # instructs Optuna to sample a floating-point value for "dropout_rate" uniformly from 0.0 to 0.5.
/work/home/ng66sume/MasterThesis/Scripts/FNN_V5/train_fnn.py:247: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  learning_rate = trial.suggest_loguniform("lr", 1e-5, 1e-2) # samples a learning rate from a log-uniform distribution
/work/home/ng66sume/MasterThesis/Scripts/FNN_V5/train_fnn.py:248: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform("weight_decay", 1e-6, 1e-2)
/work/home/ng66sume/MasterThesis/Scripts/FNN_V5/multidataloader_fnn.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  chunk_data = torch.load(chunk_file)
/home/ng66sume/miniconda3/envs/env3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
[I 2025-03-03 16:58:18,881] Trial 0 finished with value: 0.0007039545868922557 and parameters: {'num_shared_layers': 3, 'hidden_dim': 512, 'output_dim': 512, 'dropout_rate': 0.33927340787171845, 'lr': 0.0022497293394601434, 'weight_decay': 0.006835289285111554, 'batch_size': 256, 'max_output_len': 16}. Best is trial 0 with value: 0.0007039545868922557.
/work/home/ng66sume/MasterThesis/Scripts/FNN_V5/train_fnn.py:246: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.
  dropout_rate = trial.suggest_uniform("dropout_rate", 0.0, 0.5) # instructs Optuna to sample a floating-point value for "dropout_rate" uniformly from 0.0 to 0.5.
/work/home/ng66sume/MasterThesis/Scripts/FNN_V5/train_fnn.py:247: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  learning_rate = trial.suggest_loguniform("lr", 1e-5, 1e-2) # samples a learning rate from a log-uniform distribution
/work/home/ng66sume/MasterThesis/Scripts/FNN_V5/train_fnn.py:248: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  weight_decay = trial.suggest_loguniform("weight_decay", 1e-6, 1e-2)
[I 2025-03-03 17:06:01,691] Trial 1 finished with value: 0.19891193881630898 and parameters: {'num_shared_layers': 8, 'hidden_dim': 512, 'output_dim': 512, 'dropout_rate': 0.39932279540459426, 'lr': 1.1339484248290975e-05, 'weight_decay': 5.390947757330868e-06, 'batch_size': 512, 'max_output_len': 16}. Best is trial 0 with value: 0.0007039545868922557.
[I 2025-03-03 17:13:59,047] Trial 2 finished with value: 0.6715958118438721 and parameters: {'num_shared_layers': 5, 'hidden_dim': 512, 'output_dim': 1024, 'dropout_rate': 0.12736521426000025, 'lr': 0.002532421981950357, 'weight_decay': 7.572666492825345e-05, 'batch_size': 512, 'max_output_len': 64}. Best is trial 0 with value: 0.0007039545868922557.
[I 2025-03-03 17:21:57,743] Trial 3 finished with value: 0.6879972474915641 and parameters: {'num_shared_layers': 3, 'hidden_dim': 512, 'output_dim': 512, 'dropout_rate': 0.2933006930329873, 'lr': 0.009590708438373395, 'weight_decay': 7.159484264225342e-06, 'batch_size': 256, 'max_output_len': 64}. Best is trial 0 with value: 0.0007039545868922557.
[I 2025-03-03 17:29:58,968] Trial 4 finished with value: 0.0006094848421136183 and parameters: {'num_shared_layers': 5, 'hidden_dim': 512, 'output_dim': 512, 'dropout_rate': 0.20258887136935722, 'lr': 0.0006561327153717762, 'weight_decay': 0.00014355031319420122, 'batch_size': 256, 'max_output_len': 64}. Best is trial 4 with value: 0.0006094848421136183.
[I 2025-03-03 17:30:02,686] Trial 5 pruned. 
[I 2025-03-03 17:30:54,162] Trial 6 pruned. 
[I 2025-03-03 17:31:45,074] Trial 7 pruned. 
[I 2025-03-03 17:32:36,862] Trial 8 pruned. 
[I 2025-03-03 17:32:40,423] Trial 9 pruned. 
[I 2025-03-03 17:32:44,337] Trial 10 pruned. 
[I 2025-03-03 17:32:47,961] Trial 11 pruned. 
[I 2025-03-03 17:32:53,924] Trial 12 pruned. 
[I 2025-03-03 17:32:57,683] Trial 13 pruned. 
[I 2025-03-03 17:33:01,423] Trial 14 pruned. 
[I 2025-03-03 17:40:53,508] Trial 15 finished with value: 0.00044544183765538037 and parameters: {'num_shared_layers': 2, 'hidden_dim': 512, 'output_dim': 1024, 'dropout_rate': 0.27074463728080594, 'lr': 0.0039129252818050865, 'weight_decay': 0.00265491162880063, 'batch_size': 256, 'max_output_len': 16}. Best is trial 15 with value: 0.00044544183765538037.
[I 2025-03-03 17:41:09,141] Trial 16 pruned. 
[I 2025-03-03 17:41:12,884] Trial 17 pruned. 
[I 2025-03-03 17:41:16,600] Trial 18 pruned. 
[I 2025-03-03 17:41:20,216] Trial 19 pruned. 
[I 2025-03-03 17:41:23,995] Trial 20 pruned. 
[I 2025-03-03 17:49:17,114] Trial 21 finished with value: 0.00042405497931343106 and parameters: {'num_shared_layers': 2, 'hidden_dim': 512, 'output_dim': 512, 'dropout_rate': 0.3331826447010393, 'lr': 0.0037662699844675097, 'weight_decay': 0.0033337987141901847, 'batch_size': 256, 'max_output_len': 16}. Best is trial 21 with value: 0.00042405497931343106.
[I 2025-03-03 17:49:53,992] Trial 22 pruned. 
[I 2025-03-03 17:53:00,172] Trial 23 pruned. 
[I 2025-03-03 17:53:03,829] Trial 24 pruned. 
[I 2025-03-03 17:53:07,344] Trial 25 pruned. 
[I 2025-03-03 17:53:11,081] Trial 26 pruned. 
[I 2025-03-03 17:54:25,452] Trial 27 pruned. 
[I 2025-03-03 17:54:29,135] Trial 28 pruned. 
[I 2025-03-03 17:54:32,775] Trial 29 pruned. 
[I 2025-03-03 17:55:09,237] Trial 30 pruned. 
[I 2025-03-03 17:55:12,811] Trial 31 pruned. 
[I 2025-03-03 17:55:16,447] Trial 32 pruned. 
[I 2025-03-03 17:55:20,094] Trial 33 pruned. 
[I 2025-03-03 17:55:23,737] Trial 34 pruned. 
[I 2025-03-03 17:55:27,450] Trial 35 pruned. 
[I 2025-03-03 17:55:38,365] Trial 36 pruned. 
[I 2025-03-03 17:55:56,081] Trial 37 pruned. 
[I 2025-03-03 17:55:59,786] Trial 38 pruned. 
[I 2025-03-03 17:56:03,517] Trial 39 pruned. 
[I 2025-03-03 17:56:07,201] Trial 40 pruned. 
[I 2025-03-03 17:56:10,885] Trial 41 pruned. 
[I 2025-03-03 17:56:14,541] Trial 42 pruned. 
[I 2025-03-03 17:56:18,203] Trial 43 pruned. 
[I 2025-03-03 17:56:21,736] Trial 44 pruned. 
[I 2025-03-03 17:56:25,686] Trial 45 pruned. 
[I 2025-03-03 17:56:36,251] Trial 46 pruned. 
[I 2025-03-03 17:56:40,060] Trial 47 pruned. 
[I 2025-03-03 17:56:43,689] Trial 48 pruned. 
[I 2025-03-03 17:56:47,430] Trial 49 pruned. 
[I 2025-03-03 17:56:50,970] Trial 50 pruned. 
[I 2025-03-03 17:57:01,862] Trial 51 pruned. 
[I 2025-03-03 17:57:05,438] Trial 52 pruned. 
[I 2025-03-03 17:57:13,855] Trial 53 pruned. 
[I 2025-03-03 17:57:17,566] Trial 54 pruned. 
[I 2025-03-03 17:57:28,270] Trial 55 pruned. 
[I 2025-03-03 17:57:32,015] Trial 56 pruned. 
[I 2025-03-03 17:57:47,590] Trial 57 pruned. 
[I 2025-03-03 17:57:51,504] Trial 58 pruned. 
[I 2025-03-03 17:57:55,097] Trial 59 pruned. 
[I 2025-03-03 17:57:58,876] Trial 60 pruned. 
[I 2025-03-03 17:58:09,683] Trial 61 pruned. 
[I 2025-03-03 17:58:18,070] Trial 62 pruned. 
[I 2025-03-03 17:58:26,433] Trial 63 pruned. 
[I 2025-03-03 17:58:30,127] Trial 64 pruned. 
[I 2025-03-03 17:58:40,955] Trial 65 pruned. 
[I 2025-03-03 17:58:51,736] Trial 66 pruned. 
[I 2025-03-03 17:58:55,399] Trial 67 pruned. 
[I 2025-03-03 18:06:57,763] Trial 68 finished with value: 0.0003936564358549991 and parameters: {'num_shared_layers': 4, 'hidden_dim': 1024, 'output_dim': 1024, 'dropout_rate': 0.2674180457896687, 'lr': 0.0014178283014430605, 'weight_decay': 0.005456566971734418, 'batch_size': 256, 'max_output_len': 16}. Best is trial 68 with value: 0.0003936564358549991.
[I 2025-03-03 18:15:03,893] Trial 69 finished with value: 0.0004206827309514795 and parameters: {'num_shared_layers': 4, 'hidden_dim': 1024, 'output_dim': 1024, 'dropout_rate': 0.26560273952466723, 'lr': 0.0014054698869400987, 'weight_decay': 0.005095766024972705, 'batch_size': 256, 'max_output_len': 16}. Best is trial 68 with value: 0.0003936564358549991.
[I 2025-03-03 18:15:07,562] Trial 70 pruned. 
[I 2025-03-03 18:16:20,563] Trial 71 pruned. 
[I 2025-03-03 18:16:31,629] Trial 72 pruned. 
[I 2025-03-03 18:16:35,255] Trial 73 pruned. 
[I 2025-03-03 18:16:43,838] Trial 74 pruned. 
[I 2025-03-03 18:24:43,033] Trial 75 finished with value: 0.00022566731162701866 and parameters: {'num_shared_layers': 3, 'hidden_dim': 1024, 'output_dim': 1024, 'dropout_rate': 0.2493195279014973, 'lr': 0.002360175691100973, 'weight_decay': 0.0058148485830287495, 'batch_size': 256, 'max_output_len': 16}. Best is trial 75 with value: 0.00022566731162701866.
[I 2025-03-03 18:32:35,687] Trial 76 finished with value: 0.0002325878054502287 and parameters: {'num_shared_layers': 3, 'hidden_dim': 1024, 'output_dim': 1024, 'dropout_rate': 0.2527453945523886, 'lr': 0.002296923383529264, 'weight_decay': 0.005412690965673069, 'batch_size': 256, 'max_output_len': 16}. Best is trial 75 with value: 0.00022566731162701866.
[I 2025-03-03 18:32:58,352] Trial 77 pruned. 
[I 2025-03-03 18:33:02,001] Trial 78 pruned. 
[I 2025-03-03 18:33:05,758] Trial 79 pruned. 
[I 2025-03-03 18:33:14,376] Trial 80 pruned. 
[I 2025-03-03 18:41:13,144] Trial 81 finished with value: 0.0002833184969079282 and parameters: {'num_shared_layers': 3, 'hidden_dim': 1024, 'output_dim': 1024, 'dropout_rate': 0.28504976575689617, 'lr': 0.0032518842702115667, 'weight_decay': 0.006018112900945837, 'batch_size': 256, 'max_output_len': 16}. Best is trial 75 with value: 0.00022566731162701866.
[I 2025-03-03 18:41:23,907] Trial 82 pruned. 
[I 2025-03-03 18:49:12,935] Trial 83 finished with value: 0.00027894327234077664 and parameters: {'num_shared_layers': 3, 'hidden_dim': 1024, 'output_dim': 1024, 'dropout_rate': 0.30600311110860134, 'lr': 0.0019123242128774, 'weight_decay': 0.006658819717204572, 'batch_size': 256, 'max_output_len': 16}. Best is trial 75 with value: 0.00022566731162701866.
[I 2025-03-03 18:49:28,310] Trial 84 pruned. 
[I 2025-03-03 18:49:31,867] Trial 85 pruned. 
[I 2025-03-03 18:49:40,145] Trial 86 pruned. 
[I 2025-03-03 18:49:48,540] Trial 87 pruned. 
[I 2025-03-03 18:50:12,989] Trial 88 pruned. 
[I 2025-03-03 18:50:28,447] Trial 89 pruned. 
[I 2025-03-03 18:50:32,084] Trial 90 pruned. 
[I 2025-03-03 18:50:40,391] Trial 91 pruned. 
[I 2025-03-03 18:50:44,080] Trial 92 pruned. 
[I 2025-03-03 18:50:47,650] Trial 93 pruned. 
[I 2025-03-03 18:50:51,326] Trial 94 pruned. 
[I 2025-03-03 18:50:55,026] Trial 95 pruned. 
[I 2025-03-03 18:51:19,938] Trial 96 pruned. 
[I 2025-03-03 18:52:00,816] Trial 97 pruned. 
[I 2025-03-03 18:52:04,637] Trial 98 pruned. 
[I 2025-03-03 18:52:13,124] Trial 99 pruned. 
